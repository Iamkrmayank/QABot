{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FQ4pJeswxUA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "hQTCELWMxVWI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From a new Colab cell:\n",
        "!pip install --upgrade datasets transformers huggingface_hub gcsfs fsspec"
      ],
      "metadata": {
        "id": "mGCfFuRKxrFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load the SQuAD dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "# 2. Define teacher and student models\n",
        "teacher_name = \"bert-base-uncased\"\n",
        "student_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# Initialize tokenizer and models\n",
        "tokenizer = AutoTokenizer.from_pretrained(teacher_name)\n",
        "teacher_model = AutoModelForQuestionAnswering.from_pretrained(teacher_name)\n",
        "student_model = AutoModelForQuestionAnswering.from_pretrained(student_name)\n",
        "\n",
        "# 3. Preprocessing function for question answering\n",
        "max_length = 384\n",
        "doc_stride = 128\n",
        "\n",
        "def prepare_features(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['question'], examples['context'],\n",
        "        truncation='only_second', max_length=max_length,\n",
        "        stride=doc_stride, return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True, padding='max_length'\n",
        "    )\n",
        "    sample_mapping = tokenized.pop('overflow_to_sample_mapping')\n",
        "    offset_mapping = tokenized.pop('offset_mapping')\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized['input_ids'][i]\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples['answers'][sample_index]\n",
        "\n",
        "        if len(answers['answer_start']) == 0:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            start_char = answers['answer_start'][0]\n",
        "            end_char   = start_char + len(answers['text'][0])\n",
        "            # find token start/end\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # if answer not fully in span\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                start_positions.append(0)\n",
        "                end_positions.append(0)\n",
        "            else:\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                start_positions.append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                end_positions.append(token_end_index + 1)\n",
        "\n",
        "    tokenized['start_positions'] = start_positions\n",
        "    tokenized['end_positions']   = end_positions\n",
        "    return tokenized\n",
        "\n",
        "# 4. Tokenize the dataset\n",
        "tokenized_datasets = dataset.map(\n",
        "    prepare_features,\n",
        "    batched=True,\n",
        "    remove_columns=dataset['train'].column_names\n",
        ")\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 5) Tokenize the entire dataset\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "tokenized_datasets = dataset.map(\n",
        "    prepare_features,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRaPjX6MxTkj",
        "outputId": "0e85f1ed-9b41-4e78-abb8-1411b1f4f308"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Custom Trainer for distillation\n",
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, teacher_model, alpha=0.5, temperature=2.0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher_model = teacher_model\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = {\n",
        "            'start_positions': inputs.pop('start_positions'),\n",
        "            'end_positions': inputs.pop('end_positions')\n",
        "        }\n",
        "        # Student forward pass\n",
        "        outputs_student = model(**inputs, **labels)\n",
        "        loss_ce = outputs_student.loss\n",
        "\n",
        "        # Teacher forward (no gradients)\n",
        "        with torch.no_grad():\n",
        "            outputs_teacher = self.teacher_model(**inputs)\n",
        "\n",
        "        t = self.temperature\n",
        "        # Distillation losses\n",
        "        loss_start = self.kl_loss(\n",
        "            nn.functional.log_softmax(outputs_student.start_logits / t, dim=-1),\n",
        "            nn.functional.softmax(outputs_teacher.start_logits / t, dim=-1)\n",
        "        ) * (t * t)\n",
        "        loss_end = self.kl_loss(\n",
        "            nn.functional.log_softmax(outputs_student.end_logits / t, dim=-1),\n",
        "            nn.functional.softmax(outputs_teacher.end_logits / t, dim=-1)\n",
        "        ) * (t * t)\n",
        "        loss_distill = (loss_start + loss_end) / 2\n",
        "\n",
        "        # Combine CE and distillation losses\n",
        "        loss = self.alpha * loss_distill + (1 - self.alpha) * loss_ce\n",
        "        return (loss, outputs_student) if return_outputs else loss"
      ],
      "metadata": {
        "id": "V80GJY8uyaS3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Set up training arguments (without evaluation_strategy for compatibility)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./models_distilled',\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    report_to=[]\n",
        ")"
      ],
      "metadata": {
        "id": "wfnsOCYn0oPz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Initialize the distillation trainer\n",
        "trainer = DistillationTrainer(\n",
        "    teacher_model=teacher_model,\n",
        "    model=student_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BFnJp-60qyq",
        "outputId": "af3dbd9f-36a0-4b67-85a0-c8926c484d05"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-b424c080afc1>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Custom Trainer for distillation\n",
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, teacher_model, alpha=0.5, temperature=2.0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher_model = teacher_model\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        # Ensure teacher is on the same device as student\n",
        "        self.teacher_model.to(self.args.device)\n",
        "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = {\n",
        "            'start_positions': inputs.pop('start_positions'),\n",
        "            'end_positions': inputs.pop('end_positions')\n",
        "        }\n",
        "        # Student forward pass\n",
        "        outputs_student = model(**inputs, **labels)\n",
        "        loss_ce = outputs_student.loss\n",
        "\n",
        "        # Teacher forward (no gradients)\n",
        "        with torch.no_grad():\n",
        "            outputs_teacher = self.teacher_model(**inputs)\n",
        "\n",
        "        t = self.temperature\n",
        "        # Distillation losses\n",
        "        loss_start = self.kl_loss(\n",
        "            nn.functional.log_softmax(outputs_student.start_logits / t, dim=-1),\n",
        "            nn.functional.softmax(outputs_teacher.start_logits / t, dim=-1)\n",
        "        ) * (t * t)\n",
        "        loss_end = self.kl_loss(\n",
        "            nn.functional.log_softmax(outputs_student.end_logits / t, dim=-1),\n",
        "            nn.functional.softmax(outputs_teacher.end_logits / t, dim=-1)\n",
        "        ) * (t * t)\n",
        "        loss_distill = (loss_start + loss_end) / 2\n",
        "\n",
        "        # Combine CE and distillation losses\n",
        "        loss = self.alpha * loss_distill + (1 - self.alpha) * loss_ce\n",
        "        return (loss, outputs_student) if return_outputs else loss\n",
        "\n",
        "# 5. Set up training arguments (without evaluation_strategy for compatibility)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./models_distilled',\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "# 6. Initialize the distillation trainer\n",
        "trainer = DistillationTrainer(\n",
        "    teacher_model=teacher_model,\n",
        "    model=student_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# 7. Train and save the distilled student model\n",
        "trainer.train()\n",
        "trainer.save_model('./distilled_student_model')\n",
        "\n",
        "# 8. Print compression statistics\n",
        "def print_compression_stats(teacher, student):\n",
        "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
        "    student_params = sum(p.numel() for p in student.parameters())\n",
        "    print(f\"Teacher parameters: {teacher_params}\")\n",
        "    print(f\"Student parameters: {student_params}\")\n",
        "    print(f\"Compression ratio (teacher/student): {teacher_params / student_params:.2f}x\")\n",
        "\n",
        "print_compression_stats(teacher_model, student_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yY1DoKGE0q0o",
        "outputId": "828d5820-eb5b-4cd7-de2f-b0505e3c2bde"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-2a021075df8a>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n",
            "W0530 20:12:38.562000 907 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5533' max='5533' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5533/5533 53:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.401000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.738300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.562600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.396200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.356300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.248000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.246600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.204600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.153800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.139200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.151800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.138200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.113000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.116400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.102200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.112100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.125100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.074300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.061400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.073300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.070700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.068100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.054200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.071400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.004000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.993200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.025200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>1.010900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.022600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>1.009000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.982600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.017000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.014800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.995100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>1.027100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.975800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.982600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.991900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.972800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.979800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.930200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.959300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.964900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.959100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.941800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.985400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.973700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.988500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.955900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>0.960600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>0.946500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.909700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher parameters: 108893186\n",
            "Student parameters: 66364418\n",
            "Compression ratio (teacher/student): 1.64x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors='pt')\n",
        "    # remove token_type_ids if present\n",
        "    inputs = {k: v.to(trainer.args.device) for k, v in inputs.items() if k != 'token_type_ids'}\n",
        "    outputs = student_model(**inputs)\n",
        "    start_idx = torch.argmax(outputs.start_logits, dim=-1).item()\n",
        "    end_idx = torch.argmax(outputs.end_logits, dim=-1).item() + 1\n",
        "    tokens = inputs['input_ids'][0][start_idx:end_idx]\n",
        "    answer = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    # average of start/end probabilities as score\n",
        "    prob_start = torch.softmax(outputs.start_logits, dim=-1)[0][start_idx]\n",
        "    prob_end = torch.softmax(outputs.end_logits, dim=-1)[0][end_idx-1]\n",
        "    score = ((prob_start + prob_end) / 2).item()\n",
        "    return answer, score\n",
        "\n",
        "# 9. Example QA\n",
        "context = \"The Transformer architecture was introduced in the paper Attention is All You Need in 2017.\"\n",
        "question = \"When was the Transformer architecture introduced?\"\n",
        "ans, scr = answer_question(question, context)\n",
        "print(\"Answer:\", ans)\n",
        "print(\"Score:\", scr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuEi0cG70q2q",
        "outputId": "0139abc7-3f76-45e7-d6b6-50f7917e85ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 2017\n",
            "Score: 0.8541181087493896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MR51qc710q4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUU8d3Z40q6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lvQI8jtm0q-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}